# Miro backend environment example

# Database
DATABASE_URL=postgres://user:password@localhost:5432/miro

# API server
PORT=8787
# Public base URL for the Miro API (used by Better Auth for redirects and trusted origins)
AUTH_BASE_URL=http://localhost:8787

# Better Auth
# In production, set this to a strong, random secret string.
BETTER_AUTH_SECRET=change-me-in-production

# Optional social login providers
GITHUB_CLIENT_ID=
GITHUB_CLIENT_SECRET=
GOOGLE_CLIENT_ID=
GOOGLE_CLIENT_SECRET=

# AI provider configuration (backend V2)
# MIRO_AI_PROVIDER controls which chat provider is used by /v2/ai endpoints:
#   - "mock": built-in mock client (no external calls)
#   - "openai": OpenAI-compatible chat and image API using MIRO_AI_BASE_URL and MIRO_AI_API_KEY
#   - "anthropic": Anthropic chat API using ANTHROPIC_API_KEY (see velocity-stack env helpers)
#   - "local": local OpenAI-compatible gateway (e.g. a local LLM or Stable Diffusion bridge)
#
# When MIRO_AI_PROVIDER is "openai" or "local", MIRO_AI_API_KEY must be set.
# When MIRO_AI_API_KEY is empty, the backend falls back to mock mode.
MIRO_AI_PROVIDER=mock

# Base URL for an OpenAI-compatible API used by chat and image clients when an API key is set.
# For OpenAI, keep the default. For a local gateway, point this at your local server.
MIRO_AI_BASE_URL=https://api.openai.com/v1

# API key for the AI provider. Leave blank to stay in mock mode.
MIRO_AI_API_KEY=

# Logical model presets used by the frontend model switcher. These map to
# provider-specific model identifiers when MIRO_AI_PROVIDER is "openai" or "local".
MIRO_AI_MODEL_FAST=gpt-4o-mini
MIRO_AI_MODEL_BALANCED=gpt-4o
MIRO_AI_MODEL_CREATIVE=gpt-4.1-mini

# Default image model used by /v2/ai/image when no model is provided explicitly.
MIRO_AI_IMAGE_MODEL=gpt-image-1
